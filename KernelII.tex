\section{Kernels II}

Explain in each case if you would apply the kernel trick to represent your 
data. Which kind of kernel would you use? Why? If relying on kernels is not 
a good choice, which other transformation would you use?
\begin{itemize}
	\item[(a)] In this case I would apply the kernel trick because the data are 
	not	linearly separable. %FIXME
	A suitable choices for the data could be the Gaussian kernel or the Radial 
	Basis Function, that generates the desired decision boundary.\textbf{FIXME}
	
	\item[(b)] In this case, despite the data are not linearly separable, it is 
	not necessary to apply the kernel trick. Instead of using a polynomial  
	kernel, a simpler solution consists in the direct application of the map 
	$\phi(x)=[x_1, x_2, {\left\| x\right\|}^{2}]$. In the feature space the 
	classes become linearly separable and in the original space the decision 
	boundary becomes a circle.
	
	\item[(c)] In this case, the points are linearly separable, so it is not 
	necessary to apply the kernel trick.  
	
	\item[(d)] In this case I would apply the kernel trick because the data 
	are linearly non-separable. A suitable kind of kernel could be the 
	Gaussian, that kernel of degree $3$.\textbf{FIXME}
	
\end{itemize}
In the cases (a), (b) and (d), VC-dimension can be used in Structural Risk 
Minimization for choosing the kind of kernel.
