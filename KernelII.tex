\section{Kernels II}

Explain in each case if you would apply the kernel trick to represent your 
data. Which kind of kernel would you use? Why? If relying on kernels is not 
a good choice, which other transformation would you use?
\begin{itemize}
	\item[(a)] In this case, I would apply the kernel trick because the data 
	are not	linearly separable.
	A suitable choice for the data could be the Gaussian kernel that is often 
	used with a globular shape of data.
	
	\item[(b)] In this case, despite the data are not linearly separable, it is 
	not necessary to apply the kernel trick. In fact, a simple radial 
	transformation mapping, based on the distance from the centre, should be 
	enough. 
	For example, a simpler solution consists of the direct application of the 
	map $\phi(x)=[{\left\| x\right\|}^{2}]$. In the feature space, the 
	classes become linearly separable and in the original space, the decision 
	boundary becomes a circle.
	%Alternatively, using an higher dimensional space, for example relying on a 
	%polynomial of degree two or a Gaussian kernel, is not recommended because 
	%of the complexity of the model.

	\item[(c)] In this case, the points are linearly separable, so it is not 
	necessary to apply the kernel trick.  
	
	\item[(d)] In this case, I would apply the kernel trick because the data 
	are linearly non-separable. A polynomial of degree $6$ can separate the 
	classes.
	
\end{itemize}
