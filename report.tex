\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\pagestyle{fancy}

%\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Machine Learning: Assignment 2}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\usepackage{xcolor}
\usepackage{listings,lstautogobble}
\definecolor{gray}{gray}{0.5}
\colorlet{commentcolour}{green!50!black}
\colorlet{stringcolour}{red!60!black}
\colorlet{keywordcolour}{blue}
\colorlet{exceptioncolour}{yellow!50!red}
\colorlet{commandcolour}{magenta!90!black}
\colorlet{numpycolour}{blue!60!green}
\colorlet{literatecolour}{magenta!90!black}
\colorlet{promptcolour}{green!50!black}
\colorlet{specmethodcolour}{violet}

\newcommand*{\literatecolour}{\textcolor{literatecolour}}

\newcommand*{\pythonprompt}{\textcolor{promptcolour}{{>}{>}{>}}}

\lstdefinestyle{python}{
	language=python,
	showtabs=true,
	tab=,
	tabsize=4,
	basicstyle=\ttfamily\footnotesize,
	stringstyle=\color{stringcolour},
	showstringspaces=false,
	keywordstyle=\color{keywordcolour}\bfseries,
	emph={as,and,break,class,continue,def,yield,del,elif ,else,%
		except,exec,finally,for,from,global,if,in,%
		lambda,not,or,pass,print,raise,return,try,while,assert,with},
	emphstyle=\color{blue}\bfseries,
	emph={[2]True, False, None},
	emphstyle=[3]\color{commandcolour},
	morecomment=[s]{"""}{"""},
	commentstyle=\color{commentcolour}\slshape,
	emph={array, matmul, ones, T, transpose, float32},
	emphstyle=[4]\color{numpycolour},
	emph={[5]assert,yield},
	emphstyle=[5]\color{keywordcolour}\bfseries,
	emph={[6]range},
	emphstyle={[6]\color{keywordcolour}\bfseries},
	literate=*%
	{:}{{\literatecolour:}}{1}%
	{=}{{\literatecolour=}}{1}%
	{-}{{\literatecolour-}}{1}%
	{+}{{\literatecolour+}}{1}%
	{*}{{\literatecolour*}}{1}%
	{**}{{\literatecolour{**}}}2%
	{/}{{\literatecolour/}}{1}%
	{//}{{\literatecolour{//}}}2%
	{!}{{\literatecolour!}}{1}%
	{<}{{\literatecolour<}}{1}%
	{>}{{\literatecolour>}}{1}%
	{>>>}{\pythonprompt}{3},
	frame=trbl,
	rulecolor=\color{black!40},
	backgroundcolor=\color{gray!5},
	breakindent=.5\textwidth,
	frame=single,
	breaklines=true,
	basicstyle=\ttfamily\footnotesize,%
	keywordstyle=\color{keywordcolour},%
	emphstyle={[7]\color{keywordcolour}},%
	emphstyle=\color{exceptioncolour},%
	literate=*%
	{:}{{\literatecolour:}}{2}%
	{=}{{\literatecolour=}}{2}%
	{-}{{\literatecolour-}}{2}%
	{+}{{\literatecolour+}}{2}%
	{*}{{\literatecolour*}}2%
	{**}{{\literatecolour{**}}}3%
	{/}{{\literatecolour/}}{2}%
	{//}{{\literatecolour{//}}}{2}%
	{!}{{\literatecolour!}}{2}%
	{<}{{\literatecolour<}}{2}%
	{<=}{{\literatecolour{<=}}}3%
	{>}{{\literatecolour>}}{2}%
	{>=}{{\literatecolour{>=}}}3%
	{==}{{\literatecolour{==}}}3%
	{!=}{{\literatecolour{!=}}}3%
	{+=}{{\literatecolour{+=}}}3%
	{-=}{{\literatecolour{-=}}}3%
	{*=}{{\literatecolour{*=}}}3%
	{/=}{{\literatecolour{/=}}}3%
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\newenvironment{sciabstract}{%
	\begin{quote} \bf}
	{\end{quote}}

\renewcommand\refname{References and Notes}

\newcounter{problem}
\newcounter{solution}

\newcommand\Problem{%
	\stepcounter{problem}%
	\textbf{\theproblem.}~%
	\setcounter{solution}{0}%
}

\newcommand\Solution{%
	\textbf{Solution:}\\%
}

\newcommand\ASolution{%
	\stepcounter{solution}%
	\textbf{Solution \solution:}\\%
}

\begin{document}
	\thispagestyle{empty}  
	\noindent{
		\begin{tabular}{p{15cm}} 
			{\large \bf Machine Learning} \\
			Universit√† della Svizzera Italiana \\ Faculty of Informatics \\ 
			\today  \\
			\hline
			\\
		\end{tabular} 
		
		\vspace*{0.3cm} 
		
		\begin{center}
			{\Large \bf Assignment 2: Support Vector Machines}
			\vspace{2mm}
			
			{\bf Giorgia Adorni 
			(\href{mailto:giorgia.adorni@usi.ch}{giorgia.adorni@usi.ch})}
			
		\end{center}  
	}
	\vspace{0.4cm}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Kernels I}
	Consider $\mathbf{x}$, $\mathbf{y} \in \mathbb{R}^d$, $d \in \mathbb{N}$. 
	Explain in detail why each of the following functions is or is not a valid 
	kernel:\bigskip
	
	\Problem{$K(x,y)=x^Ty+(x^Ty)^2$}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{$K(x,y)=x^2e^{-y}$, $d=1$}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{$K(x,y)=ck_1(x,y)+k_2(x,y)$, where $k_1(x,y)$, $k_2(x,y)$ are 
		valid kernels in $\mathbb{R}^d$}\medskip
	
	\Solution{	}\vspace{.4cm}
	

	\section{Kernels II}
	Explain in each case if you would apply the kernel trick to represent your 
	data. Which kind of kernel would you use? Why? If relying on kernels is not 
	a good choice, which other transformation would you use?
	\begin{itemize}
		\item[(a)] ciao 
		\item[(b)] ciao 
		\item[(c)] ciao 
		\item[(d)] ciao
	\end{itemize}
	
	\section{SVMs}
	\setcounter{problem}{0}
	
	\Problem{Suppose you have 2D examples: is the decision boundary of an SVM 
	with linear kernel a straight line?}\medskip
		
	\Solution{	}\vspace{.4cm}
	
	\Problem{Suppose that the input data are linearly separable. Will an SVM 
	with linear kernel return the same parameters w regardless of the chosen 
	regularization value C? Why?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{Suppose you have 3D input examples $(x_i \in \mathbb{R}^3)$. What 
	is the dimension of the decision boundary of the SVM with linear 
	kernel?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{Is the computational effort for solving a kernel SVM increasing as 
	the dimension of the basis functions increases? Why?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{What is the maximum value of the Gaussian Kernel?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{If data are noisy and no linear boundary can perfectly classify 
	all the training data, this means we need to use a feature expansion. True 
	or false?}\medskip
	
	\Solution{	}\vspace{.4cm}
		
	\section{SVMs}
	\setcounter{problem}{0}
	Consider a linear two-class SVM classifier defined by the parameters 
	$w=[3:2]$ and $b=2$. Answer the following questions providing adequate 
	motivations:\bigskip
	
	\Problem{Is the point $x_1=[3:-1]$ classified according to the trained SVM 
	as positive?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{Assume to collect a new sample $x_1=[3:-1]$ with positive label. 
	Do you need to retrain the SVM?}\medskip
	
	\Solution{	}\vspace{.4cm}
	
	\Problem{Does the answer to the previous question apply to a generic new 
	sample?}\medskip
	
	\Solution{	}\vspace{.4cm}
			

\end{document}
