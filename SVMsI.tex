\section{SVMs}
\setcounter{problem}{0}
	
\Problem{Suppose you have 2D examples: is the decision boundary of an SVM 
with linear kernel a straight line?}\medskip

%FIXME	
\Solution{
The linear kernel assumes that the different classes are separated by a 
straight line, so the boundary is linear  in the . il decision boundary è un 
iperpiano nel feature space. in questo caso il feature space corrisponde allo 
spazio originale dei dati, quindi il decision boundary è una linea.
}\vspace{.4cm}

\Problem{Suppose that the input data are linearly separable. Will an SVM 
with linear kernel return the same parameters $w$ regardless of the chosen 
regularization value C? Why?}\medskip

\Solution{
Even if the data are linearly separable, a Soft Margin SVM using a linear 
kernel could not return the same parameters $w$ regardless of the chosen value 
of $C$. In fact, reducing the values of $C$, the margin increases and some 
training points close to the margin can be misclassified.
}\vspace{.4cm}

\Problem{Suppose you have 3D input examples $(x_i \in \mathbb{R}^3)$. What 
is the dimension of the decision boundary of the SVM with linear 
kernel?}\medskip

\Solution{\textbf{FIXME}Since we are in a 3D space, the decision boundary 
should be a 2d 
plane. }\vspace{.4cm}

\Problem{Is the computational effort for solving a kernel SVM increasing as 
the dimension of the basis functions increases? Why?}\medskip

\Solution{
No, thanks to the ``kernel trick'' even if there is an infinite-dimensional 
basis, we need to compute only the kernel, which only has a low dimension. 

%The computational cost might increase if the computation of the inner products 
%is costly.
}\vspace{.4cm}

\Problem{What is the maximum value of the Gaussian Kernel?}\medskip

\Solution{Since the Gaussian kernel corresponds to $\exp \{ - \frac{\| x - 
x'\|^2}{2 \sigma ^2} \}$, that is an exponential with a negative argument, 
since the function decreases, the maximum value occurs when $x=0$, and it is 
1.}\vspace{.4cm}

\Problem{If data are noisy and no linear boundary can perfectly classify 
all the training data, this means we need to use a feature expansion. True 
or false?}\medskip

\Solution{
	\textbf{FIXME}
FALSE As in any statistical problem, we will always do better on the training 
data if we use a feature expansion, but that does not mean we will improve the 
test error. Not all regression lines should perfectly interpolate all the 
training points, and not all classifiers should perfectly classify all the 
training data
}\vspace{.4cm}
