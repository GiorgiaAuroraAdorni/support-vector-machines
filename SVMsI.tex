\section{SVMs}
\setcounter{problem}{0}
	
\Problem{Suppose you have 2D examples: is the decision boundary of an SVM 
with linear kernel a straight line?}\medskip

%FIXME	
\Solution{
The classic linear kernel assumes that the different classes are separated by a 
straight line.
The boundary is the same one a perceptron would produce, thus it is linear in 
the feature space.
}\vspace{.4cm}

\Problem{Suppose that the input data are linearly separable. Will an SVM 
with linear kernel return the same parameters $w$ regardless of the chosen 
regularization value C? Why?}\medskip

\Solution{
If the data are linearly separable, an SVM using a linear kernel will return the
same parameters w regardless of the chosen value of C.
}\vspace{.4cm}

\Problem{Suppose you have 3D input examples $(x_i \in \mathbb{R}^3)$. What 
is the dimension of the decision boundary of the SVM with linear 
kernel?}\medskip

\Solution{	}\vspace{.4cm}

\Problem{Is the computational effort for solving a kernel SVM increasing as 
the dimension of the basis functions increases? Why?}\medskip

\Solution{
No, thanks to the "kernel trick" even if there is an infinite-dimensional 
basis, we need the $N^2$ inner products between training data points. The 
computational cost might increase if the computation of the inner products is 
costly.
}\vspace{.4cm}

\Problem{What is the maximum value of the Gaussian Kernel?}\medskip

\Solution{The maximum value of the Gaussian kernel is 1.}\vspace{.4cm}

\Problem{If data are noisy and no linear boundary can perfectly classify 
all the training data, this means we need to use a feature expansion. True 
or false?}\medskip

\Solution{
FALSE As in any statistical problem, we will always do better on the training 
data if we use a feature expansion, but that does not mean we will improve the 
test error. Not all regression lines should perfectly interpolate all the 
training points, and not all classifiers should perfectly classify all the 
training data
}\vspace{.4cm}
